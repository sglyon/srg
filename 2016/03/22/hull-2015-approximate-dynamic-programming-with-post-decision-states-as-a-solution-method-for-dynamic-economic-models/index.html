<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Hull (2015) (Approximate dynamic programming with post-decision states as a solution method for dynamic economic models)  &middot; Sargent Reading Group Notes</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="Hull (2015) (Approximate dynamic programming with post-decision states as a solution method for dynamic economic models)  &middot; Sargent Reading Group Notes ">
<meta property="og:site_name" content="Sargent Reading Group Notes"/>
<meta property="og:url" content="http://srg.spencerlyon.com/2016/03/22/hull-2015-approximate-dynamic-programming-with-post-decision-states-as-a-solution-method-for-dynamic-economic-models/" />
<meta property="og:locale" content="en-EN">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2016-03-22T00:00:00Z" />
<meta property="og:article:modified_time" content="2016-03-22T00:00:00Z" />

  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@" />
<meta name="twitter:title" content="Hull (2015) (Approximate dynamic programming with post-decision states as a solution method for dynamic economic models)" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="http://srg.spencerlyon.com/2016/03/22/hull-2015-approximate-dynamic-programming-with-post-decision-states-as-a-solution-method-for-dynamic-economic-models/" />
<meta name="twitter:domain" content="http://srg.spencerlyon.com/">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Hull (2015) (Approximate dynamic programming with post-decision states as a solution method for dynamic economic models)",
    "author": {
      "@type": "Person",
      "name": "http://profiles.google.com/+?rel=author"
    },
    "datePublished": "2016-03-22",
    "description": "",
    "wordCount": 853
  }
</script>


<link rel="canonical" href="http://srg.spencerlyon.com/2016/03/22/hull-2015-approximate-dynamic-programming-with-post-decision-states-as-a-solution-method-for-dynamic-economic-models/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://srg.spencerlyon.com/touch-icon-144-precomposed.png">
<link href="http://srg.spencerlyon.com/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.32-DEV" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="http://srg.spencerlyon.com/css/font-awesome.min.css">
<link rel="stylesheet" href="http://srg.spencerlyon.com/css/style.css">


  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="http://srg.spencerlyon.com/">
  srg

</a>

</div>

  
<div class="container topline">
  
  Spencer Lyon


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="http://srg.spencerlyon.com/">Home</a>


  

</nav>

<div class="container nav secondary no-print">
  
<a id="contact-link-email" class="contact_link" href="mailto:spencer.lyon@stern.nyu.edu">
  <span class="fa fa-envelope-square"></span><span>email</span></a>



















</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>Hull (2015) (Approximate dynamic programming with post-decision states as a solution method for dynamic economic models)
</h1>

  <div class="metas">
<time datetime="2016-03-22">22 Mar, 2016</time>


  
  &middot; Read in about 5 min
  &middot; (853 Words)
  <br>
  


</div>

</header>

  <div class="container content">
  <p>This paper presents a stochastic simulation method for solving dynamic economic models.</p>
<p>The ideas in this paper lean on a literature sometimes known as approximate dynamic programming and can enable us to solve models with many state variables and non-convexities in objectives and constraints.</p>
<p>My intent is to summarize the core theoretical ideas behind the algorithm.</p>
<h2 id="main-idea-post-decision-states">Main idea: Post decision states</h2>
<h3 id="notation-classic-bellman-equation">Notation: classic Bellman equation</h3>
<p>Consider a stationary economic model where at time <span class="math inline"><em>t</em></span> the state is summarized by a vector <span class="math inline"><em>s</em><sub><em>t</em></sub></span> of endogenous state variables and a vector <span class="math inline"><em>x</em><sub><em>t</em></sub></span> of exogenous state variables.</p>
<p>The optimization problem of an agent is often summarized by a Bellman equation of the form</p>
<p><br /><span class="math display"><em>V</em>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em></sub>) = <em>m</em><em>a</em><em>x</em><sub><em>c</em><sub><em>t</em></sub></sub><em>u</em>(<em>c</em><sub><em>t</em></sub>) + <em>β</em><em>E</em>[<em>V</em>(<em>s</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 1</sub>)]</span><br /></p>
<p>subject to</p>
<p><br /><span class="math display"><em>s</em><sub><em>t</em> + 1</sub> = <em>f</em>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em></sub>, <em>c</em><sub><em>t</em></sub>), <em>c</em><sub><em>t</em></sub> ∈ <em>Γ</em>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em></sub>), <em>x</em><sub><em>t</em> + 1</sub> = <em>g</em>(<em>x</em><sub><em>t</em></sub>, <em>ϵ</em><sub><em>t</em> + 1</sub>)</span><br /></p>
<h3 id="post-decision-state-value-function">Post-decision state value function</h3>
<p>Note that that the transition function for the endogenous state takes <span class="math inline"><em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em></sub></span>, and <span class="math inline"><em>c</em><sub><em>t</em></sub></span> and emits <span class="math inline"><em>s</em><sub><em>t</em> + 1</sub></span>. We can think of <span class="math inline"><em>s</em><sub><em>t</em> + 1</sub></span> being chosen at the end of period <span class="math inline"><em>t</em></span>, meaning after the controls <span class="math inline"><em>c</em><sub><em>t</em></sub></span> have been decided.</p>
<p>In the standard (or pre-decision) Bellman equation, we have that the state at time <span class="math inline"><em>t</em></span> is the endogenous state defined at the end of period <span class="math inline"><em>t</em> − 1</span> and the exogenous state realized at the start of period <span class="math inline"><em>t</em></span>.</p>
<p>We will now consider a different representation of the state vector that couples the endogenous state defined at the end of period <span class="math inline"><em>t</em> − 1</span> with the exogenous state realized at the start of period <span class="math inline"><em>t</em> − 1</span>. That is we will consider <span class="math inline"><em>s</em><sub><em>t</em></sub></span> and <span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>, which is known as the post-decision state at time <span class="math inline"><em>t</em> − 1</span>.</p>
<p>Let <span class="math inline"><em>V</em><sup><em>x</em></sup>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em> − 1</sub>)</span> be the value of having post-decision state <span class="math inline"><em>s</em><sub><em>t</em></sub></span>, <span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span> in period <span class="math inline"><em>t</em> − 1</span>. This is the maximum expected, discounted utility an agent can achieve after controls have been selected in period <span class="math inline"><em>t</em> − 1</span>.</p>
<p>Because <span class="math inline"><em>c</em><sub><em>t</em> − 1</sub></span> is not chosen until <em>after</em> <span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span> is realized, we know that <span class="math inline"><em>V</em><sup><em>x</em></sup>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em> − 1</sub>)</span> is equal to the expectation of the maximum expected, discounted utility the agent will receive after <span class="math inline"><em>x</em><sub><em>t</em></sub></span> arrives. That is, we can write</p>
<p><br /><span class="math display"><em>V</em><sup><em>x</em></sup>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em> − 1</sub>) = <em>E</em>[<em>V</em>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em></sub>)|<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em> − 1</sub>],</span><br /></p>
<p>where <span class="math inline"><em>V</em>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em></sub>)</span> is the pre-decision Bellman equation.</p>
<p>It follows that we can write</p>
<p><br /><span class="math display"><em>V</em>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em></sub>) = <em>m</em><em>a</em><em>x</em><sub><em>c</em><sub><em>t</em></sub></sub> + <em>β</em><em>V</em><sup><em>x</em></sup>(<em>s</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em></sub>).</span><br /></p>
<p>These equations can be manipulated to produce the recursive form of the post-decision state Bellman equation:</p>
<p><br /><span class="math display"><em>V</em><sup><em>x</em></sup>(<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em> − 1</sub>) = <em>E</em>[<em>m</em><em>a</em><em>x</em><sub><em>c</em><sub><em>t</em></sub></sub><em>u</em>(<em>c</em><sub><em>t</em></sub>) + <em>β</em><em>V</em><sup><em>x</em></sup>(<em>s</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em></sub>)|<em>s</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em> − 1</sub>].</span><br /></p>
<p>Notice that the expectation is <em>outside</em> the max operator, meaning that the maximization problem is deterministic.</p>
<h2 id="algorithm">Algorithm</h2>
<p>Now that we have the post-decision state Bellman equation, the algorithm is fairly straightforward. I will present the algorithm from the paper in the context of Markov exogenous processes, but I believe it is incorrectly specified. I’ll discuss how I’d change it later.</p>
<ol type="1">
<li>Setup
<ul>
<li>Discretize endogenous state space</li>
<li>Choose a simulation length <span class="math inline"><em>T</em></span></li>
<li>Choose initial endogenous and exogenous states</li>
<li>Construct an initial guess for the value function at the discritized endogenous and exogenous states.</li>
</ul></li>
<li>Iterations
<ul>
<li>Construct a time series of Exogenous states for t=1, 2, …, T</li>
<li>For time <span class="math inline"><em>t</em> = 1, 2, ..., <em>T</em></span> perform the following 3 steps:
<ol type="1">
<li>Choose controls <span class="math inline"><em>c</em><sub><em>t</em></sub></span> to maximize the term inside the expectation on the RHS of <span class="math inline"><em>V</em>(<em>s</em><sub><em>t</em></sub>, <em>x</em><em>t</em> − 1)</span>. To do this we need to using the value function from the previous iteration for the future value function</li>
<li>Compute the expectation implicitly by updating the guess of the value function using a convex combination of the previous iteration’s value function and the value computed above</li>
<li>Using the chosen controls and realization of exogenous state, apply the endogenous transition equation to iterate the endogenous state forward one period</li>
</ol></li>
</ul></li>
<li>Convergence:
<ul>
<li>Check a convergence criterion that compares the discretized value function across multiple iterations.</li>
<li>If converged, return the discretized value function and run a regression on the time series to obtain a policy function from the time series of controls</li>
</ul></li>
</ol>
<h3 id="comments-on-the-algorithm">Comments on the algorithm</h3>
<p>Here are a few comments about the algorithm:</p>
<ul>
<li>Because the expectation operator is outside the max operator, we don’t have to spend time computing expectations when solving the optimization problem in each period of the simulation. This speeds up computation quite a bit.</li>
<li>Expectations are computed implicitly when we update our guess for the post-decision state value function</li>
</ul>
<p><strong>My compliant</strong> I think it is incorrect to re-generate a time series of exogenous states on each iteration. Doing so will not allow the algorithm to ever converge as the updated value function in iteration <span class="math inline"><em>n</em></span> is dependent on the randomness from the exogenous simulation in period <span class="math inline"><em>n</em></span>.</p>
<p>Using the same simulated time series for exogenous states in every iteration (as in other simulation algorithms in the literature) will allow this algorithm to converge; subject to a particular exogenous path. To ensure that the solution is accurate for the underlying data generating process and not just the simulation you use, make sure that the length of the time series is large.</p>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    
    <a class="prev" href="http://srg.spencerlyon.com/2016/03/08/achdou-han-lasry-lions-moll-2015-hetergenous-agent-models-in-continuous-time/" title="Achdou, Han, Lasry, Lions, Moll (2015) (Hetergenous agent models in continuous time)">
      Previous
    </a>
    

    
    <a class="next" href="http://srg.spencerlyon.com/2016/03/29/de-farias--van-roy-2003-the-linear-programming-approach-to-approximate-dynamic-programming/" title="de Farias &amp; van Roy (2003) (The Linear Programming Approach to Approximate Dynamic Programming)">
      Next
    </a>
    

  


</div>

  <div class="container comments">
  <h2>Comments</h2>
  

</div>

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  

</div>


  
<div class="container copyright">
  
  &copy; 2015 Spencer Lyon.


</div>


</div>

</footer>

    </main>
    





    
  </body>
</html>

