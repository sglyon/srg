<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Konda and Tsitsiklis (2003) (Actor-critic Algorithms)  &middot; Sargent Reading Group Notes</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="Konda and Tsitsiklis (2003) (Actor-critic Algorithms)  &middot; Sargent Reading Group Notes ">
<meta property="og:site_name" content="Sargent Reading Group Notes"/>
<meta property="og:url" content="http://srg.spencerlyon.com/2016/04/19/konda-and-tsitsiklis-2003-actor-critic-algorithms/" />
<meta property="og:locale" content="en-EN">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2016-04-19T00:00:00Z" />
<meta property="og:article:modified_time" content="2016-04-19T00:00:00Z" />

  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@" />
<meta name="twitter:title" content="Konda and Tsitsiklis (2003) (Actor-critic Algorithms)" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="http://srg.spencerlyon.com/2016/04/19/konda-and-tsitsiklis-2003-actor-critic-algorithms/" />
<meta name="twitter:domain" content="http://srg.spencerlyon.com/">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Konda and Tsitsiklis (2003) (Actor-critic Algorithms)",
    "author": {
      "@type": "Person",
      "name": "http://profiles.google.com/+?rel=author"
    },
    "datePublished": "2016-04-19",
    "description": "",
    "wordCount":  741 
  }
</script>



<link rel="canonical" href="http://srg.spencerlyon.com/2016/04/19/konda-and-tsitsiklis-2003-actor-critic-algorithms/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://srg.spencerlyon.com/touch-icon-144-precomposed.png">
<link href="http://srg.spencerlyon.com/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.16-DEV" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="http://srg.spencerlyon.com/css/font-awesome.min.css">
<link rel="stylesheet" href="http://srg.spencerlyon.com/css/style.css">


  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="http://srg.spencerlyon.com/">
  srg

</a>

</div>

  
<div class="container topline">
  
  Spencer Lyon


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="http://srg.spencerlyon.com/">Home</a>


  

</nav>

<div class="container nav secondary no-print">
  
<a id="contact-link-email" class="contact_link" href="mailto:spencer.lyon@stern.nyu.edu">
  <span class="fa fa-envelope-square"></span><span>email</span></a>



















</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>Konda and Tsitsiklis (2003) (Actor-critic Algorithms)
</h1>

  <div class="metas">
<time datetime="2016-04-19">19 Apr, 2016</time>


  
  &middot; Read in about 4 min
  &middot; (741 Words)
  <br>
  


</div>

</header>

  <div class="container content">
  <p><h2 id="background">Background</h2>
<p>When a Markov decision process (MDP) is formulated as a dynamic programming problem, the reinforcement learning literature proposes are two classic classes of algorithms to solve them. Let&rsquo;s briefly review these types of algorithms and point out strengths and weaknesses of each.</p>
<h4 id="actor-only-methods">1: Actor only methods</h4>
<p>We can think of an actor as a fictitious character that operates on a policy rule.</p>
<p>When I talk about the performance of an actor, I mean the value of following a policy.</p>
<p>These methods are often implemented by estimating the gradient of the performance of an actor using simulation.</p>
<p>There are two main issues with these &quot;policy iteration&quot;-esque algorithms:</p>
<ol style="list-style-type: decimal">
<li>Gradient estimators can have high variance</li>
<li>As the policy changes, a new gradient is estimated independently. This means there is no sense of learning from past data.</li>
</ol>
<h4 id="critic-only-methods">2: Critic only methods</h4>
<p>We can think of a critic operating on either a Q or V value function.</p>
<p>These methods rely exclusively on value function approximation and try to learn the solution to Bellman&rsquo;s equation.</p>
<p>The main issues with critic only methods are:</p>
<ol style="list-style-type: decimal">
<li>They are indirect in that they do not try to optimize directly over the policy space</li>
<li>Even with an accurate approximation of the value function, results that guarantee the near-optimality of the corresponding policy are difficult to guarantee.</li>
</ol>
<h2 id="main-idea">Main idea</h2>
<p>This paper suggests two actor-critic hybrid methods that aim to maintain the best features of each algorithm, while overcoming the shortcomings mentioned above.</p>
<p>The main idea behind the algorithms is that the critic uses a linearly parameterized approximation scheme and simulation to learn a value function. The actor then uses the learned information to update parameters on the policy function in a direction of performance improvement.</p>
<p><em>Aside to tie back to econ</em>: This feels like modified policy iteration or Howard&rsquo;s improvement algorithm, but it is different in a few ways:</p>
<ol style="list-style-type: decimal">
<li>There is a learning element to these algorithms, which means we don&rsquo;t have to compute expectations explicitly.</li>
<li>We will be learning Q functions, which describe the value of being in a state and taking any feasible action (instead of the V function that describes the value of being in a state and choosing the optimal action).</li>
</ol>
<h2 id="algorithms">Algorithms</h2>
<!-- The algorithms are presented in terms of identifying a randomized stationary
policy. We typically think of a policy function as assigning a unique action to
each state. A randomized stationary policy attaches a probability distribution
over actions to each state. In some sense a standard policy is like a pure
strategy, whereas a randomized stationary is like a mixed strategy. -->
<p>The presentation is very technical and relies on assumptions that aren&rsquo;t necessarily applicable to the models we write down, so I won&rsquo;t the paper exactly as it was written. Instead, I will sketch the algorithm and explain the key insight the authors have that makes the algorithm tractable.</p>
<h3 id="setup">Setup</h3>
<p>We will represent the critic using three variables:</p>
<ul>
<li>A coefficient vector of length <span class="math inline"><em>m</em></span> that describe a linear parametrization of Q in terms of basis functions.</li>
<li>A scalar <span class="math inline"><em>Î±</em></span> that represents the average value of following the actor&rsquo;s policy</li>
<li>A vector of length <span class="math inline"><em>m</em></span> that represents Sutton&rsquo;s eligibility trace. This vector is used to form a bridge between fixed point methods and Monte Carlo methods</li>
</ul>
<p>The actor is represented by a suitable parametric representation of the policy function.</p>
<h3 id="algo">Algo</h3>
<p>In order to understand the algorithm, I need to provide two related definitions. A <strong>temporal difference</strong> is the difference between the current approximation of a variable and a realization of that variable. In other words it is the error in our approximation for a particular sample.</p>
<p>We say we <em>update</em> parameters or approximations using a <strong>temporal difference</strong> if the new approximation is the sum of the current approximation and a scaled temporal difference.</p>
<p>The algorithm <em>roughly</em> proceeds as follows:</p>
<ul>
<li>Initialize the actor and critic</li>
<li>Perform one step updates of the actor and critic as follows:
<ul>
<li>Because we learn Q, we enter a time period with a particular state and action in hand</li>
<li>The actor will dictate a new action and we need to simulate a new state, potentially given that action</li>
<li>The critic&rsquo;s parameters are updated according to:
<ul>
<li>Average value of policy: temporal difference update (using flow implied by state and action)</li>
<li>Coefficient vector for Q: temporal difference update, scaled by eligibility trace</li>
<li>Eligibility trace:</li>
</ul></li>
<li>The actor&rsquo;s parameter vector is updated using a gradient approach that resembles Newton&rsquo;s method. It takes into account updates to the actor</li>
</ul></li>
</ul>
<p>The key insight the authors have that make this algorithm tractable is the following:</p>
<p>Actors have to update a small number of parameters compared to the number of states. So the critic doesn&rsquo;t need to form an approximation over the entire domain of Q, but rather a special projection of the Q onto the space spanned by the actor&rsquo;s parameter vector.</p>
<div id="refs" class="references"></p>

<p></div></p>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    
    <a class="prev" href="http://srg.spencerlyon.com/2016/04/12/busoniu-deschutter-babuska2010-approximate-dynamic-programming-and-reinforcement-learning/" title="Busoniu, DeSchutter, Babuska(2010) (Approximate dynamic programming and reinforcement learning)">
      Previous
    </a>
    

    
    <a class="next" href="http://srg.spencerlyon.com/2016/04/26/timoshenko-2015-product-switching-in-a-model-of-learning/" title="Timoshenko (2015) (Product switching in a model of learning)">
      Next
    </a>
    

  


</div>

  <div class="container comments">
  <h2>Comments</h2>
  

</div>

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  

</div>


  
<div class="container copyright">
  
  &copy; 2015 Spencer Lyon.


</div>


</div>

</footer>

    </main>
    





    
  </body>
</html>

